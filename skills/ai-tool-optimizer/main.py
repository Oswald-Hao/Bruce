#!/usr/bin/env python3
"""
AIå·¥å…·ä¼˜åŒ–å™¨ - ä¸»ç¨‹åº
"""

import argparse
import json
import os
from typing import Dict, List, Any

from optimizer import PromptOptimizer
from token_analyzer import TokenAnalyzer
from model_selector import ModelSelector
from cache_manager import CacheManager
from quality_evaluator import QualityEvaluator


class AIToolOptimizer:
    """AIå·¥å…·ä¼˜åŒ–å™¨"""

    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.config = self.load_config(config_path)
        self.prompt_optimizer = PromptOptimizer(self.config)
        self.token_analyzer = TokenAnalyzer(self.config)
        self.model_selector = ModelSelector(self.config)
        self.cache_manager = CacheManager(self.config)
        self.quality_evaluator = QualityEvaluator(self.config)

    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            "models": {
                "gpt-4": {
                    "cost_per_1k_input": 0.03,
                    "cost_per_1k_output": 0.06,
                    "max_tokens": 8192
                },
                "gpt-3.5-turbo": {
                    "cost_per_1k_input": 0.0005,
                    "cost_per_1k_output": 0.0015,
                    "max_tokens": 4096
                },
                "claude-3-opus": {
                    "cost_per_1k_input": 0.015,
                    "cost_per_1k_output": 0.075,
                    "max_tokens": 200000
                }
            },
            "budget": {
                "monthly_limit": 1000,
                "daily_limit": 50
            },
            "storage": {
                "usage_db": "usage_db.json",
                "cache_db": "cache_db.json",
                "quality_db": "quality_db.json"
            }
        }

        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            return default_config

    def optimize_prompt(
        self,
        prompt: str,
        model: str = None,
        task_type: str = None
    ) -> Dict[str, Any]:
        """
        ä¼˜åŒ–æç¤ºè¯

        Args:
            prompt: åŸå§‹æç¤ºè¯
            model: ç›®æ ‡æ¨¡å‹
            task_type: ä»»åŠ¡ç±»å‹

        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        return self.prompt_optimizer.optimize(prompt, model, task_type)

    def analyze_tokens(
        self,
        file_path: str = None,
        days: int = 7
    ) -> Dict[str, Any]:
        """
        åˆ†æTokenä½¿ç”¨æƒ…å†µ

        Args:
            file_path: ä½¿ç”¨æ—¥å¿—æ–‡ä»¶
            days: ç»Ÿè®¡å¤©æ•°

        Returns:
            åˆ†æç»“æœ
        """
        return self.token_analyzer.analyze(file_path, days)

    def suggest_model(
        self,
        task: str,
        budget: float = None,
        quality_priority: bool = False
    ) -> Dict[str, Any]:
        """
        æ¨èæ¨¡å‹

        Args:
            task: ä»»åŠ¡æè¿°
            budget: é¢„ç®—
            quality_priority: æ˜¯å¦ä¼˜å…ˆè€ƒè™‘è´¨é‡

        Returns:
            æ¨èç»“æœ
        """
        return self.model_selector.suggest(task, budget, quality_priority)

    def analyze_cache(self, file_path: str = None) -> Dict[str, Any]:
        """
        åˆ†æç¼“å­˜ä½¿ç”¨æƒ…å†µ

        Args:
            file_path: ç¼“å­˜æ—¥å¿—æ–‡ä»¶

        Returns:
            åˆ†æç»“æœ
        """
        return self.cache_manager.analyze(file_path)

    def evaluate_quality(self, file_path: str = None) -> Dict[str, Any]:
        """
        è¯„ä¼°å“åº”è´¨é‡

        Args:
            file_path: å“åº”æ—¥å¿—æ–‡ä»¶

        Returns:
            è¯„ä¼°ç»“æœ
        """
        return self.quality_evaluator.evaluate(file_path)

    def get_usage_report(self, days: int = 7) -> Dict[str, Any]:
        """
        è·å–ä½¿ç”¨æŠ¥å‘Š

        Args:
            days: ç»Ÿè®¡å¤©æ•°

        Returns:
            ä½¿ç”¨æŠ¥å‘Š
        """
        token_analysis = self.analyze_tokens(None, days)
        quality_report = self.evaluate_quality(None)

        return {
            'token_usage': token_analysis,
            'quality': quality_report,
            'total_cost': token_analysis.get('total_cost', 0),
            'optimization_suggestions': self._generate_suggestions(token_analysis, quality_report)
        }

    def _generate_suggestions(
        self,
        token_analysis: Dict,
        quality_report: Dict
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # Tokenä¼˜åŒ–å»ºè®®
        if token_analysis.get('potential_savings', 0) > 0:
            savings = token_analysis['potential_savings']
            suggestions.append(f"ğŸ’° é€šè¿‡ç¼“å­˜å¯ä»¥èŠ‚çœçº¦{savings:.2f}å…ƒ")

        if token_analysis.get('avg_tokens_per_request', 0) > 2000:
            suggestions.append("ğŸ“ å¹³å‡Tokenä½¿ç”¨é‡åé«˜ï¼Œå»ºè®®ä¼˜åŒ–æç¤ºè¯é•¿åº¦")

        # è´¨é‡ä¼˜åŒ–å»ºè®®
        if quality_report.get('avg_quality_score', 1.0) < 0.8:
            suggestions.append("âš ï¸  å“åº”è´¨é‡ä½äºæ ‡å‡†ï¼Œå»ºè®®æ£€æŸ¥æç¤ºè¯")

        if quality_report.get('error_rate', 0) > 0.05:
            suggestions.append("ğŸ”§ é”™è¯¯ç‡åé«˜ï¼Œå»ºè®®å¢åŠ é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")

        return suggestions


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='AIå·¥å…·ä¼˜åŒ–å™¨')
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # ä¼˜åŒ–æç¤ºè¯å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize-prompt', help='ä¼˜åŒ–æç¤ºè¯')
    optimize_parser.add_argument('--prompt', required=True, help='æç¤ºè¯')
    optimize_parser.add_argument('--model', help='ç›®æ ‡æ¨¡å‹')
    optimize_parser.add_argument('--task', help='ä»»åŠ¡ç±»å‹')

    # Tokenåˆ†æå‘½ä»¤
    token_parser = subparsers.add_parser('analyze-tokens', help='åˆ†æTokenä½¿ç”¨')
    token_parser.add_argument('--file', help='ä½¿ç”¨æ—¥å¿—æ–‡ä»¶')
    token_parser.add_argument('--days', type=int, default=7, help='ç»Ÿè®¡å¤©æ•°')

    # æ¨¡å‹æ¨èå‘½ä»¤
    model_parser = subparsers.add_parser('suggest-model', help='æ¨èæ¨¡å‹')
    model_parser.add_argument('--task', required=True, help='ä»»åŠ¡æè¿°')
    model_parser.add_argument('--budget', type=float, help='é¢„ç®—')
    model_parser.add_argument('--quality-priority', action='store_true', help='ä¼˜å…ˆè´¨é‡')

    # ç¼“å­˜åˆ†æå‘½ä»¤
    cache_parser = subparsers.add_parser('analyze-cache', help='åˆ†æç¼“å­˜')
    cache_parser.add_argument('--file', help='ç¼“å­˜æ—¥å¿—æ–‡ä»¶')

    # è´¨é‡è¯„ä¼°å‘½ä»¤
    quality_parser = subparsers.add_parser('evaluate-quality', help='è¯„ä¼°è´¨é‡')
    quality_parser.add_argument('--file', help='å“åº”æ—¥å¿—æ–‡ä»¶')

    # ä½¿ç”¨æŠ¥å‘Šå‘½ä»¤
    report_parser = subparsers.add_parser('report', help='ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š')
    report_parser.add_argument('--days', type=int, default=7, help='ç»Ÿè®¡å¤©æ•°')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨
    optimizer = AIToolOptimizer()

    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'optimize-prompt':
        result = optimizer.optimize_prompt(
            prompt=args.prompt,
            model=args.model,
            task_type=args.task
        )

        print("\nğŸ“ æç¤ºè¯ä¼˜åŒ–ç»“æœ:")
        print(f"\nåŸå§‹æç¤ºè¯:\n{result['original_prompt']}")
        print(f"\nä¼˜åŒ–åæç¤ºè¯:\n{result['optimized_prompt']}")
        print(f"\næ”¹è¿›ç‚¹:")
        for improvement in result['improvements']:
            print(f"  â€¢ {improvement}")

        if 'estimated_token_reduction' in result:
            print(f"\né¢„ä¼°Tokenå‡å°‘: {result['estimated_token_reduction']:.1f}%")
            print(f"é¢„ä¼°æˆæœ¬èŠ‚çœ: {result['estimated_cost_savings']:.4f}å…ƒ")

    elif args.command == 'analyze-tokens':
        result = optimizer.analyze_tokens(args.file, args.days)

        print(f"\nğŸ“Š Tokenä½¿ç”¨åˆ†æ (æœ€è¿‘{args.days}å¤©):")
        print(f"  æ€»è¯·æ±‚æ•°: {result['total_requests']}")
        print(f"  æ€»Tokenæ•°: {result['total_tokens']:,}")
        print(f"  å¹³å‡Token/è¯·æ±‚: {result['avg_tokens_per_request']:.1f}")
        print(f"  æ€»æˆæœ¬: {result['total_cost']:.4f}å…ƒ")
        print(f"  æ—¥å‡æˆæœ¬: {result['daily_cost']:.4f}å…ƒ")

        if result.get('by_model'):
            print(f"\n  å„æ¨¡å‹ä½¿ç”¨:")
            for model, data in result['by_model'].items():
                print(f"    {model}: {data['requests']}æ¬¡, {data['tokens']:,}tokens, {data['cost']:.4f}å…ƒ")

        if result.get('potential_savings', 0) > 0:
            print(f"\nğŸ’° æ½œåœ¨èŠ‚çœ: {result['potential_savings']:.4f}å…ƒï¼ˆé€šè¿‡ç¼“å­˜ï¼‰")

    elif args.command == 'suggest-model':
        result = optimizer.suggest_model(
            task=args.task,
            budget=args.budget,
            quality_priority=args.quality_priority
        )

        print(f"\nğŸ¯ æ¨¡å‹æ¨èç»“æœ:")
        print(f"  ä»»åŠ¡: {result['task']}")
        print(f"\næ¨èæ¨¡å‹: {result['recommended_model']}")
        print(f"  åŸå› : {result['reason']}")
        print(f"  é¢„ä¼°æˆæœ¬: {result['estimated_cost']:.4f}å…ƒ")
        print(f"  è´¨é‡è¯„åˆ†: {result['quality_score']:.1f}/10")

        if result.get('alternatives'):
            print(f"\nå¤‡é€‰æ¨¡å‹:")
            for alt in result['alternatives']:
                print(f"  â€¢ {alt['model']}: {alt['cost']:.4f}å…ƒ, è´¨é‡{alt['quality']:.1f}/10 - {alt['reason']}")

    elif args.command == 'analyze-cache':
        result = optimizer.analyze_cache(args.file)

        print(f"\nğŸ’¾ ç¼“å­˜åˆ†æ:")
        print(f"  ç¼“å­˜æ¡ç›®æ•°: {result['cache_entries']}")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {result['hit_rate']:.1%}")
        print(f"  é¿å…çš„APIè°ƒç”¨: {result['api_calls_avoided']}")
        print(f"  æˆæœ¬èŠ‚çœ: {result['cost_savings']:.4f}å…ƒ")

        if result.get('most_common', 0) > 0:
            print(f"\n  æœ€å¸¸ç”¨çš„ç¼“å­˜é”®: {result['most_common']}æ¬¡")

    elif args.command == 'evaluate-quality':
        result = optimizer.evaluate_quality(args.file)

        print(f"\nâœ¨ è´¨é‡è¯„ä¼°:")
        print(f"  æ€»å“åº”æ•°: {result['total_responses']}")
        print(f"  å¹³å‡è´¨é‡è¯„åˆ†: {result['avg_quality_score']:.2f}/1.0")
        print(f"  é”™è¯¯ç‡: {result['error_rate']:.1%}")

        if result.get('by_quality'):
            print(f"\n  è´¨é‡åˆ†å¸ƒ:")
            for level, count in result['by_quality'].items():
                print(f"    {level}: {count}æ¬¡")

        if result.get('common_issues'):
            print(f"\n  å¸¸è§é—®é¢˜:")
            for issue in result['common_issues'][:5]:
                print(f"    â€¢ {issue}")

    elif args.command == 'report':
        report = optimizer.get_usage_report(args.days)

        print(f"\nğŸ“‹ AIå·¥å…·ä½¿ç”¨æŠ¥å‘Š (æœ€è¿‘{args.days}å¤©):")
        print(f"\nğŸ’° æˆæœ¬ç»Ÿè®¡:")
        print(f"  æ€»æˆæœ¬: {report['total_cost']:.4f}å…ƒ")
        print(f"  Tokenæ€»æ•°: {report['token_usage']['total_tokens']:,}")
        print(f"  å¹³å‡Token/è¯·æ±‚: {report['token_usage']['avg_tokens_per_request']:.1f}")

        print(f"\nâœ¨ è´¨é‡è¯„ä¼°:")
        print(f"  å¹³å‡è¯„åˆ†: {report['quality']['avg_quality_score']:.2f}")
        print(f"  é”™è¯¯ç‡: {report['quality']['error_rate']:.1%}")

        if report['optimization_suggestions']:
            print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            for suggestion in report['optimization_suggestions']:
                print(f"  {suggestion}")


if __name__ == '__main__':
    main()
