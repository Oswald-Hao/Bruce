#!/usr/bin/env python3
"""
AIå·¥å…·ä¼˜åŒ–å™¨ - æµ‹è¯•å¥—ä»¶
"""

import unittest
import os
import json
import shutil

# å¯¼å…¥ä¸»æ¨¡å—
from main import AIToolOptimizer
from optimizer import PromptOptimizer
from token_analyzer import TokenAnalyzer
from model_selector import ModelSelector
from cache_manager import CacheManager
from quality_evaluator import QualityEvaluator


class TestPromptOptimizer(unittest.TestCase):
    """æç¤ºè¯ä¼˜åŒ–å™¨æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        config = {
            'models': {
                'gpt-4': {
                    'cost_per_1k_input': 0.03
                },
                'gpt-3.5-turbo': {
                    'cost_per_1k_input': 0.0005
                }
            },
            'storage': {}
        }
        self.optimizer = PromptOptimizer(config)

    def test_optimize_simple_prompt(self):
        """æµ‹è¯•1: ä¼˜åŒ–ç®€å•æç¤ºè¯"""
        prompt = "è¯·å¸®æˆ‘è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
        result = self.optimizer.optimize(prompt)

        self.assertIsNotNone(result)
        self.assertIn('optimized_prompt', result)
        self.assertEqual(result['original_prompt'], prompt)
        self.assertTrue(len(result['improvements']) > 0)

        print("âœ“ æµ‹è¯•1é€šè¿‡: ç®€å•æç¤ºè¯ä¼˜åŒ–æˆåŠŸ")

    def test_optimize_long_prompt(self):
        """æµ‹è¯•2: ä¼˜åŒ–é•¿æç¤ºè¯"""
        prompt = "æˆ‘å¸Œæœ›ä½ èƒ½å¸®æˆ‘è§£é‡Šäººå·¥æ™ºèƒ½" * 20
        result = self.optimizer.optimize(prompt)

        self.assertIsNotNone(result)
        self.assertLess(result['optimized_token_count'], result['original_token_count'])

        print("âœ“ æµ‹è¯•2é€šè¿‡: é•¿æç¤ºè¯ä¼˜åŒ–æˆåŠŸ")

    def test_optimize_with_task_type(self):
        """æµ‹è¯•3: å¸¦ä»»åŠ¡ç±»å‹çš„ä¼˜åŒ–"""
        prompt = "å†™ä¸€ä¸ªå‡½æ•°"
        result = self.optimizer.optimize(prompt, task_type='code')

        optimized = result['optimized_prompt']
        self.assertIn('ä»£ç ', optimized) or self.assertIn('function', optimized)

        print("âœ“ æµ‹è¯•3é€šè¿‡: å¸¦ä»»åŠ¡ç±»å‹ä¼˜åŒ–æˆåŠŸ")

    def test_token_count(self):
        """æµ‹è¯•4: Tokenè®¡æ•°"""
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•"
        count = self.optimizer._count_tokens(text)

        self.assertGreater(count, 0)

        print("âœ“ æµ‹è¯•4é€šè¿‡: Tokenè®¡æ•°æˆåŠŸ")


class TestTokenAnalyzer(unittest.TestCase):
    """Tokenåˆ†æå™¨æµ‹è¯•"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»å‡†å¤‡"""
        cls.test_dir = "test_token_data"
        cls.original_dir = os.getcwd()
        os.makedirs(cls.test_dir, exist_ok=True)
        os.chdir(cls.test_dir)

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        os.chdir(cls.original_dir)
        if os.path.exists(cls.test_dir):
            shutil.rmtree(cls.test_dir)

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        config = {
            'models': {
                'gpt-3.5-turbo': {
                    'cost_per_1k_input': 0.0005
                }
            },
            'storage': {
                'usage_db': 'test_usage_db.json'
            }
        }
        self.analyzer = TokenAnalyzer(config)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        if os.path.exists('test_usage_db.json'):
            os.remove('test_usage_db.json')

    def test_analyze_usage(self):
        """æµ‹è¯•5: åˆ†æä½¿ç”¨æƒ…å†µ"""
        result = self.analyzer.analyze(days=7)

        self.assertIn('total_requests', result)
        self.assertIn('total_tokens', result)
        self.assertIn('total_cost', result)

        print("âœ“ æµ‹è¯•5é€šè¿‡: ä½¿ç”¨åˆ†ææˆåŠŸ")

    def test_record_usage(self):
        """æµ‹è¯•6: è®°å½•ä½¿ç”¨"""
        self.analyzer.record_usage(
            model='gpt-3.5-turbo',
            tokens=1000,
            cost=0.001,
            prompt_hash='abc123'
        )

        result = self.analyzer.analyze()
        self.assertEqual(result['total_requests'], 1)

        print("âœ“ æµ‹è¯•6é€šè¿‡: ä½¿ç”¨è®°å½•æˆåŠŸ")

    def test_get_top_prompts(self):
        """æµ‹è¯•7: è·å–å¸¸ç”¨æç¤ºè¯"""
        # è®°å½•å¤šä¸ªä½¿ç”¨
        for i in range(10):
            self.analyzer.record_usage(
                model='gpt-3.5-turbo',
                tokens=1000,
                cost=0.001,
                prompt_hash='hash1' if i < 5 else f'hash{i}'
            )

        top_prompts = self.analyzer.get_top_prompts()
        self.assertGreater(len(top_prompts), 0)
        self.assertEqual(top_prompts[0]['count'], 5)

        print("âœ“ æµ‹è¯•7é€šè¿‡: å¸¸ç”¨æç¤ºè¯è·å–æˆåŠŸ")


class TestModelSelector(unittest.TestCase):
    """æ¨¡å‹é€‰æ‹©å™¨æµ‹è¯•"""

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        config = {
            'models': {
                'gpt-4': {
                    'cost_per_1k_input': 0.03
                },
                'gpt-3.5-turbo': {
                    'cost_per_1k_input': 0.0005
                }
            },
            'storage': {}
        }
        self.selector = ModelSelector(config)

    def test_suggest_model_for_code(self):
        """æµ‹è¯•8: ä»£ç ä»»åŠ¡æ¨¡å‹æ¨è"""
        result = self.selector.suggest(task='ç¼–å†™Pythonä»£ç ', budget=0.01)

        self.assertIn('recommended_model', result)
        self.assertIn('quality_score', result)
        self.assertIn('estimated_cost', result)

        print("âœ“ æµ‹è¯•8é€šè¿‡: ä»£ç ä»»åŠ¡æ¨¡å‹æ¨èæˆåŠŸ")

    def test_suggest_model_with_budget(self):
        """æµ‹è¯•9: æœ‰é¢„ç®—é™åˆ¶çš„æ¨¡å‹æ¨è"""
        result = self.selector.suggest(
            task='åˆ†ææ•°æ®',
            budget=0.001  # ä½é¢„ç®—ï¼Œåº”è¯¥æ¨èä¾¿å®œæ¨¡å‹
        )

        # ä½é¢„ç®—åº”è¯¥æ¨èä¾¿å®œçš„æ¨¡å‹
        self.assertIn('recommended_model', result)

        print("âœ“ æµ‹è¯•9é€šè¿‡: æœ‰é¢„ç®—é™åˆ¶çš„æ¨èæˆåŠŸ")

    def test_quality_priority(self):
        """æµ‹è¯•10: è´¨é‡ä¼˜å…ˆæ¨¡å¼"""
        result = self.selector.suggest(
            task='å¤æ‚åˆ†æ',
            quality_priority=True
        )

        self.assertEqual(result['recommended_model'], 'gpt-4')  # è´¨é‡ä¼˜å…ˆ

        print("âœ“ æµ‹è¯•10é€šè¿‡: è´¨é‡ä¼˜å…ˆæ¨¡å¼æˆåŠŸ")

    def test_identify_task_type(self):
        """æµ‹è¯•11: ä»»åŠ¡ç±»å‹è¯†åˆ«"""
        self.assertEqual(
            self.selector._identify_task_type('ç¼–å†™ä»£ç '),
            'code'
        )
        self.assertEqual(
            self.selector._identify_task_type('å†™ä¸€ç¯‡æ–‡ç« '),
            'writing'
        )

        print("âœ“ æµ‹è¯•11é€šè¿‡: ä»»åŠ¡ç±»å‹è¯†åˆ«æˆåŠŸ")


class TestCacheManager(unittest.TestCase):
    """ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»å‡†å¤‡"""
        cls.test_dir = "test_cache_data"
        cls.original_dir = os.getcwd()
        os.makedirs(cls.test_dir, exist_ok=True)
        os.chdir(cls.test_dir)

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        os.chdir(cls.original_dir)
        if os.path.exists(cls.test_dir):
            shutil.rmtree(cls.test_dir)

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        config = {
            'storage': {
                'cache_db': 'test_cache_db.json'
            }
        }
        self.cache = CacheManager(config)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        if os.path.exists('test_cache_db.json'):
            os.remove('test_cache_db.json')

    def test_cache_set_and_get(self):
        """æµ‹è¯•12: ç¼“å­˜è®¾ç½®å’Œè·å–"""
        prompt = "æµ‹è¯•æç¤ºè¯"
        model = "gpt-3.5-turbo"
        response = "æµ‹è¯•å“åº”"

        # è®¾ç½®ç¼“å­˜
        self.cache.set(prompt, model, response, 100)

        # è·å–ç¼“å­˜
        cached = self.cache.get(prompt, model)

        self.assertIsNotNone(cached)
        self.assertEqual(cached['response'], response)
        self.assertTrue(cached['from_cache'])

        print("âœ“ æµ‹è¯•12é€šè¿‡: ç¼“å­˜è®¾ç½®å’Œè·å–æˆåŠŸ")

    def test_cache_miss(self):
        """æµ‹è¯•13: ç¼“å­˜æœªå‘½ä¸­"""
        result = self.cache.get("ä¸å­˜åœ¨çš„æç¤ºè¯", "gpt-3.5-turbo")

        self.assertIsNone(result)

        print("âœ“ æµ‹è¯•13é€šè¿‡: ç¼“å­˜æœªå‘½ä¸­å¤„ç†æ­£ç¡®")

    def test_cache_analyze(self):
        """æµ‹è¯•14: ç¼“å­˜åˆ†æ"""
        # æ·»åŠ ä¸€äº›ç¼“å­˜
        for i in range(5):
            self.cache.set(f"æç¤ºè¯{i}", "gpt-3.5-turbo", f"å“åº”{i}", 100)

        # å‘½ä¸­ä¸€äº›ç¼“å­˜
        self.cache.get("æç¤ºè¯0", "gpt-3.5-turbo")
        self.cache.get("æç¤ºè¯1", "gpt-3.5-turbo")

        analysis = self.cache.analyze()
        self.assertEqual(analysis['cache_entries'], 5)
        self.assertEqual(analysis['total_hits'], 2)

        print("âœ“ æµ‹è¯•14é€šè¿‡: ç¼“å­˜åˆ†ææˆåŠŸ")


class TestQualityEvaluator(unittest.TestCase):
    """è´¨é‡è¯„ä¼°å™¨æµ‹è¯•"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»å‡†å¤‡"""
        cls.test_dir = "test_quality_data"
        cls.original_dir = os.getcwd()
        os.makedirs(cls.test_dir, exist_ok=True)
        os.chdir(cls.test_dir)

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        os.chdir(cls.original_dir)
        if os.path.exists(cls.test_dir):
            shutil.rmtree(cls.test_dir)

    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        config = {
            'storage': {
                'quality_db': 'test_quality_db.json'
            }
        }
        self.evaluator = QualityEvaluator(config)

    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        if os.path.exists('test_quality_db.json'):
            os.remove('test_quality_db.json')

    def test_evaluate_good_response(self):
        """æµ‹è¯•15: è¯„ä¼°å¥½çš„å“åº”"""
        prompt = "ä»€ä¹ˆæ˜¯AIï¼Ÿ"
        response = "äººå·¥æ™ºèƒ½æ˜¯æŒ‡ç”±è®¡ç®—æœºç³»ç»Ÿè¡¨ç°å‡ºçš„æ™ºèƒ½..."

        result = self.evaluator.evaluate_response(prompt, response)

        self.assertGreater(result['quality_score'], 0.7)
        self.assertFalse(result['is_error'])

        print("âœ“ æµ‹è¯•15é€šè¿‡: å¥½å“åº”è¯„ä¼°æˆåŠŸ")

    def test_evaluate_empty_response(self):
        """æµ‹è¯•16: è¯„ä¼°ç©ºå“åº”"""
        result = self.evaluator.evaluate_response("é—®é¢˜", "")

        self.assertEqual(result['quality_score'], 0.0)
        self.assertTrue(result['is_error'])

        print("âœ“ æµ‹è¯•16é€šè¿‡: ç©ºå“åº”è¯„ä¼°æˆåŠŸ")

    def test_evaluate_short_response(self):
        """æµ‹è¯•17: è¯„ä¼°è¿‡çŸ­å“åº”"""
        prompt = "è¯·è¯¦ç»†è§£é‡Šäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"
        response = "AIå‘å±•å¾ˆå¿«"

        result = self.evaluator.evaluate_response(prompt, response, expected_length=500)

        self.assertLess(result['quality_score'], 0.9)
        self.assertIn('å“åº”è¿‡çŸ­', result['issues'])

        print("âœ“ æµ‹è¯•17é€šè¿‡: è¿‡çŸ­å“åº”è¯„ä¼°æˆåŠŸ")

    def test_quality_analyze(self):
        """æµ‹è¯•18: è´¨é‡åˆ†æ"""
        # è®°å½•ä¸€äº›å“åº”
        for i in range(10):
            self.evaluator.record_response(
                prompt=f"é—®é¢˜{i}",
                response=f"å“åº”{i}" * 10,
                quality_score=0.8 + (i % 3) * 0.1,
                issues=[]
            )

        analysis = self.evaluator.evaluate()
        self.assertEqual(analysis['total_responses'], 10)
        self.assertGreater(analysis['avg_quality_score'], 0.7)

        print("âœ“ æµ‹è¯•18é€šè¿‡: è´¨é‡åˆ†ææˆåŠŸ")


class TestIntegration(unittest.TestCase):
    """é›†æˆæµ‹è¯•"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç±»å‡†å¤‡"""
        cls.test_dir = "test_integration_data"
        cls.original_dir = os.getcwd()
        os.makedirs(cls.test_dir, exist_ok=True)
        os.chdir(cls.test_dir)

    @classmethod
    def tearDownClass(cls):
        """æµ‹è¯•ç±»æ¸…ç†"""
        os.chdir(cls.original_dir)
        if os.path.exists(cls.test_dir):
            shutil.rmtree(cls.test_dir)

    def test_complete_workflow(self):
        """æµ‹è¯•19: å®Œæ•´å·¥ä½œæµç¨‹"""
        optimizer = AIToolOptimizer()

        # 1. ä¼˜åŒ–æç¤ºè¯
        prompt_result = optimizer.optimize_prompt(
            prompt="è¯·å¸®æˆ‘è§£é‡Šäººå·¥æ™ºèƒ½",
            task_type="writing"
        )
        self.assertTrue(len(prompt_result['improvements']) > 0)

        # 2. Tokenåˆ†æ
        token_result = optimizer.analyze_tokens()
        self.assertIn('total_requests', token_result)

        # 3. æ¨¡å‹æ¨è
        model_result = optimizer.suggest_model(task="ç¼–å†™Pythonä»£ç ", budget=0.01)
        self.assertIn('recommended_model', model_result)

        # 4. ç¼“å­˜åˆ†æ
        cache_result = optimizer.analyze_cache()
        self.assertIn('cache_entries', cache_result)

        # 5. è´¨é‡è¯„ä¼°
        quality_result = optimizer.evaluate_quality()
        self.assertIn('avg_quality_score', quality_result)

        # 6. ç”ŸæˆæŠ¥å‘Š
        report = optimizer.get_usage_report()
        self.assertIn('token_usage', report)
        self.assertIn('quality', report)

        print("âœ“ æµ‹è¯•19é€šè¿‡: å®Œæ•´å·¥ä½œæµç¨‹æˆåŠŸ")


if __name__ == '__main__':
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromModule(__import__(__name__))

    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # è¾“å‡ºæµ‹è¯•æ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ‘˜è¦")
    print("="*60)
    print(f"æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"å¤±è´¥: {len(result.failures)}")
    print(f"é”™è¯¯: {len(result.errors)}")

    if result.wasSuccessful():
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
